{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c253ec6d",
   "metadata": {},
   "source": [
    "# Project - Realtime Restructure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a08f9e",
   "metadata": {},
   "source": [
    "## introduction：\n",
    "\n",
    "The project is to combine two input frames under the same scene to generate a large landscape model view.<br>\n",
    "The program will take two device video inputs as the source, then perform feature matching based on similar parts of the input source, and after that rotate the input frame by using the matched feature points to achieve the function of recombination.\n",
    "\n",
    "<br>\n",
    "\n",
    "More details can be visited through:\n",
    "- GitHub: https://github.com/D18130495/Video-Restructure\n",
    "- Yuhong He Blog: https://imageprocessing.yuhong.me/\n",
    "- Yushun Zeng Blog: https://imageprocessing.huaruoyumu.com/\n",
    "- Xinyu Zhang Blog: https://imageprocessing633246182.wordpress.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a90746d",
   "metadata": {},
   "source": [
    "## User guide:\n",
    "\n",
    "Setup the equipments:\n",
    "1. Two video input equipment connected to the program run machine.\n",
    "\n",
    "<br>\n",
    "\n",
    "Run the program:\n",
    "1. Make sure that the two video input device input views have some similarity under the same scenes.\n",
    "2. Run the program.\n",
    "3. Move two video input devices wider or narrower to find the best angle you want the output to be.\n",
    "4. Click on the \"result\" window to recombine two input videos again."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01adf0d1",
   "metadata": {},
   "source": [
    "## Project workflow and used algorithms:\n",
    "\n",
    "1. Ask system open two videos to capture input videos:<br>\n",
    "Use cv2.VideoCapture to get two webcam inputs and use while loop to continuously show the frame.\n",
    "\n",
    "<br>\n",
    "\n",
    "2. Detect key points of two video inputs with OpenCV SIFT algorithm and calculate feature description information:<br>\n",
    "Use cv2.xfeatures2d.SIFT_create() to create SIFT with .detectAndCompute() to calculate the key points and features.\n",
    "After having the result of the key points, perusing each point to get the X, Y for each point and store them in an array.\n",
    "\n",
    "<br>\n",
    "\n",
    "3. Match the detected keypoints:<br>\n",
    "Use cv2.BFMatcher() to create the Brute-force matcher, with cv2.knnMatch to match the detected keypoints.\n",
    "\n",
    "<br>\n",
    "\n",
    "4. Filtering out the best-matched keypoints:<br>\n",
    "Use cv2.drawMatchesKnn() to draw the relationship between two input frames for the not filtered keypoints, and use the appropriate parameter to filter out the best-matched keypoint, after that draw the relationship again with the filtered keypoints.\n",
    "\n",
    "<br>\n",
    "\n",
    "5. Using filtered keypoints with homography matrix to rotate input frame:<br>\n",
    "Use cv2.findHomography() with filtered keypoints to rotate one of the input frames, and after that put another frame on the rotated frame, and the two frames will be combined.\n",
    "\n",
    "<br>\n",
    "\n",
    "6. Stabilize the output and add mouse click event to make the program can restructure two frames again:<br>\n",
    "Use variables to store whether the result has been calculated once, if the result already exists program will not calculate again, and use the mouse click call back function to make the program can calculate the result again to restructure the two frames again."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9f3936",
   "metadata": {},
   "source": [
    "## Citation of peer-reviewed research:\n",
    "\n",
    "Comparison with other design peer-reviewed research:\n",
    "\n",
    "[1] Rosebrock, A. (2021) Image inpainting with opencv and python, PyImageSearch. Available at: https://pyimagesearch.com/2020/05/18/image-inpainting-with-opencv-and-python/ [Accessed: 7 October, 2022].\n",
    "\n",
    "\n",
    "[2] Rosebrock, A. (2021) Multi-class object detection and bounding box regression with Keras, tensorflow, and Deep Learning, PyImageSearch. Available at: https://pyimagesearch.com/2020/10/12/multi-class-object-detection-and-bounding-box-regression-with-keras-tensorflow-and-deep-learning/ [Accessed: 7 October, 2022].\n",
    "\n",
    "\n",
    "[3] Rosebrock, A. (2021) OpenCV text detection (East text detector), PyImageSearch. Available at: https://pyimagesearch.com/2018/08/20/opencv-text-detection-east-text-detector/ [Accessed: 7 October, 2022].\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "Project peer-reviewed research:\n",
    "\n",
    "[1] Introduction to SIFT (Scale-Invariant Feature Transform).\n",
    "Available at: https://docs.opencv.org/4.x/da/df5/tutorial_py_sift_intro.html [accessed 16 October, 2022].\n",
    "\n",
    "[2] Feature Matching. Available at: https://opencv24-python-tutorials.readthedocs.io/en/latest/py_tutorials/py_feature2d/py_matcher/py_matcher.html [accessed 16 October, 2022].\n",
    "\n",
    "[3] Evaluating OpenCV’s new RANSACs.\n",
    "Available at: https://opencv.org/evaluating-opencvs-new-ransacs/ [accessed 16 October, 2022].\n",
    "\n",
    "[4] Basic concepts of the homography explained with code.\n",
    "Available at: https://docs.opencv.org/4.x/d9/dab/tutorial_homography.html [accessed 16 October, 2022].\n",
    "\n",
    "[5] Feature Detection OpenCV.\n",
    "Available at: https://docs.opencv.org/4.x/d7/d66/tutorial_feature_detection.html [accessed 31 October, 2022].\n",
    "\n",
    "[6] Feature Matching OpenCV.\n",
    "Available at: https://docs.opencv.org/4.x/dc/dc3/tutorial_py_matcher.html [accessed 31 October, 2022].\n",
    "\n",
    "[7] Pless, R. and Let (no date) Lecture 3: Camera rotations and homographies - ppt download, SlidePlayer. Available at: https://slideplayer.com/slide/14746373/ [Accessed: 16 November, 2022].\n",
    "\n",
    "[8] Feature Matching + Homography to find Objects. Available at: https://docs.opencv.org/3.4/d1/de0/tutorial_py_feature_homography.html\n",
    "[accessed 16 November, 2022]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "354af795",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 # OpenCV\n",
    "import numpy as np # numpy use to handle Homography rotation parameter type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e43aec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ask system open two videos to capture input videos\n",
    "view_one = cv2.VideoCapture(1)\n",
    "view_two = cv2.VideoCapture(0)\n",
    "\n",
    "# set the cached to None when first time run the program and global for the mouse click function callback method\n",
    "global cachedH\n",
    "cachedH = None\n",
    "\n",
    "# continuously showing the frame\n",
    "while True:\n",
    "    ret, frame_one = view_one.read()\n",
    "    ret, frame_two = view_two.read()\n",
    "    \n",
    "    # resize two input frames\n",
    "    frame_one = cv2.resize(frame_one, (600, 400))\n",
    "    frame_two = cv2.resize(frame_two, (600, 400))\n",
    "    \n",
    "    # cachedH can stabilize the result, by not always calculate rotate parameter H for each loop\n",
    "    # cachedH is None means this is the first time run the program\n",
    "    # or the user clicks to let the program recombine the input frames again\n",
    "    if cachedH is None:\n",
    "        # First step: detect the keypoints\n",
    "        # convert input frames to gray color space for better keypoint detection \n",
    "        frame_one_gray = cv2.cvtColor(frame_one, cv2.COLOR_BGR2GRAY)\n",
    "        frame_two_gray = cv2.cvtColor(frame_two, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # create SIFT calculator\n",
    "        calculator = cv2.xfeatures2d.SIFT_create()\n",
    "        \n",
    "        # detect key points and calculate feature description information\n",
    "        frame_one_keypoints, frame_one_features = calculator.detectAndCompute(frame_one_gray, None)\n",
    "        frame_two_keypoints, frame_two_features = calculator.detectAndCompute(frame_two_gray, None)\n",
    "\n",
    "        # demo of detected keypoints drawn on the frame\n",
    "#         keypoints_frame_one = cv2.drawKeypoints(frame_one, frame_one_keypoints, None, color=(0, 255, 0), \n",
    "#                                                 flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "#         keypoints_frame_two = cv2.drawKeypoints(frame_two, frame_two_keypoints, None, color=(0, 255, 0), \n",
    "#                                                 flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "        \n",
    "        \n",
    "#         cv2.imshow('Keypoints frame one', keypoints_frame_one)\n",
    "#         cv2.imshow('Keypoints frame two', keypoints_frame_two)\n",
    "    \n",
    "\n",
    "        # create two arrays to store detected keypoints\n",
    "        frame_one_keypoints_array = []\n",
    "        frame_two_keypoints_array = []\n",
    "\n",
    "        # traverse the detected keypoints and store in the array\n",
    "        for keypoint_one in frame_one_keypoints:\n",
    "            frame_one_keypoints_array.append(keypoint_one.pt)\n",
    "\n",
    "        for keypoint_two in frame_two_keypoints:\n",
    "            frame_two_keypoints_array.append(keypoint_two.pt)\n",
    "    \n",
    "    \n",
    "        # Second step: match the keypoints\n",
    "        # create Brute-force descriptor matcher\n",
    "        descriptor_matcher = cv2.BFMatcher()\n",
    "\n",
    "        # use BFMatcher.knnMatch() to get k best matches\n",
    "        matches = descriptor_matcher.knnMatch(frame_one_features, frame_two_features, k = 2)\n",
    "        \n",
    "        # demo of matched keypoints drawn on the frame\n",
    "#         matched_line = cv2.drawMatchesKnn(frame_one, frame_one_keypoints, frame_two, frame_two_keypoints, matches, None, \n",
    "#                                   flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "\n",
    "#         cv2.imshow(\"Matched line\", matched_line)\n",
    "        \n",
    "        # create the array use to store the best-matched keypoints\n",
    "        matched_points = []\n",
    "\n",
    "        # traverse the matched keypoints and store the matched keypoints\n",
    "        # if m.distance < 0.75 * n.distance means best matched\n",
    "        for m, n in matches:\n",
    "            if m.distance < 0.75 * n.distance:\n",
    "                matched_points.append([m])\n",
    "\n",
    "        # demo of the best matched keypoints drawn on the frame     \n",
    "#         filtered_matched_line = cv2.drawMatchesKnn(frame_one, frame_one_keypoints, frame_two, frame_two_keypoints,\n",
    "#                                                    matched_points, None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "\n",
    "#         cv2.imshow(\"Filtered matched line\", filtered_matched_line)\n",
    "\n",
    "\n",
    "        # create two arrays to store the best matched keypoints on the original frames\n",
    "        frame_one_best_keypoints_array = []\n",
    "        frame_two_best_keypoints_array = []\n",
    "\n",
    "        # cv2.findHomography() need at least four kaypoints to rotate the frame\n",
    "        # if there are more then 4 best matched keypoints, find each keypoint on the original frame then store them in arrays\n",
    "        if len(matched_points) > 4:\n",
    "            for best_keypoint_one in matched_points:\n",
    "                point = best_keypoint_one[0].queryIdx\n",
    "                frame_one_best_keypoints_array.append(frame_one_keypoints_array[point])\n",
    "\n",
    "            for best_keypoint_two in matched_points:\n",
    "                point = best_keypoint_two[0].trainIdx\n",
    "                frame_two_best_keypoints_array.append(frame_two_keypoints_array[point])\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        # use the found best matched keypoints on the original frame with Homography to calculate rotate parameter H\n",
    "        H, status = cv2.findHomography(np.float32(frame_one_best_keypoints_array), \n",
    "                                       np.float32(frame_two_best_keypoints_array), cv2.RANSAC, 4.0)\n",
    "\n",
    "        # cached the calculated rotate parameter H, which can make the result Stable\n",
    "        cachedH = H\n",
    "\n",
    "    # apply the rotation on one of the frame with parameter H, and overlap another frame on the rotated frame\n",
    "    rotated = cv2.warpPerspective(frame_one, cachedH, (frame_one.shape[1] + frame_two.shape[1], frame_one.shape[0]))\n",
    "    result = rotated.copy()\n",
    "    result[0:frame_two.shape[0], 0:frame_two.shape[1]] = frame_two\n",
    "    \n",
    "    # mouse click callback function which use to clear cachedH\n",
    "    # make the program calculate rotate parameter H again and reapply the rotation on the input frame\n",
    "    def draw(event, x, y, flags, param):\n",
    "        if event == cv2.EVENT_LBUTTONDOWN:\n",
    "            global cachedH\n",
    "            cachedH = None\n",
    "\n",
    "    cv2.namedWindow(\"result\") # mouse click window\n",
    "    cv2.setMouseCallback(\"result\", draw) # mouse click callback\n",
    "    \n",
    "    # show the original two video input\n",
    "    cv2.imshow(\"view one\", frame_one)\n",
    "    cv2.imshow(\"view two\", frame_two)\n",
    "    \n",
    "    # show the combined result\n",
    "    cv2.imshow(\"result\", result)\n",
    "    \n",
    "    # stop the videos by pressing q  \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# release two webcams and destroy windows\n",
    "view_one.release()\n",
    "view_two.release()\n",
    "cv2.destroyAllWindows() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
